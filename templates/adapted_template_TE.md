# Wang & Strong (1997) Template Adapted by TE

Formulate into logical questions that can be answered with Yes/No for summarising and scoring.

## Intrinsic DQ (7)

- **Accuracy**
	* I1 Does the reliability of the annotations meet the typical norms? (N, expertise, reporting inter-rater agreement, elimination of responses or annotators)
	* I2 Are instances explicitly linked with all forms of data (e.g., connected by a unique id)?
	* I3 Is annotation done to the _precision_ commensurate with the theory (e.g., likert scales for dimensions, is there an explicit theory behind annotations)?
- **Objectivity**
	* I4 Does the annotation scheme adhere to the theory it follows and can it be interpreted by others in the same way?
- **Believability**
	* I5 Is the documentation and the instructions (specific wording that the annotators saw) for the annotation available?
	* I6 Is the overall credibility of the data at the level required (e.g., sufficient power per instance to provide a reliable annotation, no conflicting reporting, or piece-meal or ad-hoc assembly of multiple incompatible or unclear dataset versions)?
- **Reputation**
	* I7 Has the dataset been used by multiple studies by different authors (over 3-5)?

## Accessibility DQ (4)

- **Accessibility** (public availability, discoverability of resources required to reproduce; music availability)
	* A1 Is the dataset publicly available and functionally useable (all the scripts needed to compile separate parts of the data are available)?
	* A2 Is the music available? (functionally available, but see next question)
- **Access security**
	* A3 If parts of the data (music, lyrics, album covers, etc) are copyrighted, is computerized data accessible with reasonable effort?
- **Cost effectiveness** (from 1996 paper)
	* What has been the estimated cost of establishing the dataset (annotator pay x time) [non-scored question]
	* A4 Is the value of the data significant in terms of the cost invested (e.g. over $5000) or culture-specific expertise? [scored variant of the question]

## Contextual DQ (6)

- **Relevancy** (width of the coverage: annotation, features, music, lyrics, demographics, etc., what was sampling strategy from GEBRU)
	* C1 Is a sampling strategy for choosing the excerpts provided and explained?
	* C2 Is the scope of the dataset clearly defined and justified (e.g. "Western classical piano music" rather than emotional music)?
	* C3 Is the background of the annotators relevant for the music and tasks annotated?
- **Timeliness**
	* C4 Can the data obtained and used without human intervention in a timely manner?
- **Value-added**
	* C5 Does the dataset fill in a clear gap in the field, is it genuinely novel? 
- **Completeness**
	* C5 Are all the items complete (no missing sub-sections, items, annotations, domains of data)?
- **Amount of data**
	* What are the numbers for (1) stimulus N, (2) stimulus length (median), (3) annotator N (if raw data given), (4) N domains of data (eg., annotations, audio, lyrics, features, videos, movement, tags, etc.) [non-scored question]

## Representational DQ (7)

- **Interpretability & ease of understanding**
	* R1 Are all annotated constructs defined in the documentation?
	* R2 Is the documentation complete and the data human readable?
- **Concise representation** (merge/build procedure; preprocessing; outlier removal)
	* R3 Are the building blocks of the dataset explained clearly and the preprocessing steps explained or covered explicitly with scripts/functions?
- **Consistent representation** (what emotion framework is represented and how)
	* R4 Are domain of data (annotations, music, lyrics, tags, features) explained where they come from and how were they processed, extracted and/or compiled?
	* R5 Do the emotion annotations represent a known theoretical notion and what framework (affective circumplex, affect quadrants, basic emotions, aesthetic emotions, other)  
- **Type of acquisition of data** (crowd-sourced, experts, university students, lay people, gamified, etc.)
	* R6 Are the annotator credentials explained at a sufficient level (crowd-sourced, experts, university students, laypeople, gamified, etc.)?
	* R7 Is the _data description_ (see next) fully covered in the documentation?

## Data description

	- Amount of data (see above)
	- Genre(s)
	- Domains of data
	- Annotator expertise and culture
	- Annotation task
	- Excerpt duration
	- Source where available

Note: there are 7, 4, 6, and 7 questions for the four categories now, but some of them overlap.

### Scoring and weighting scheme

Sum of each category (intrinsic, accessible, contextual, representational) could be 1, for a total of 4.

* * *

#### Example scoring: DEAM

##### Intrinsic DQ (7)

- **Accuracy**
	* I1 Does the reliability of the annotations meet the typical norms? **Yes?**
	* I2 Are instances explicitly linked with all forms of data (e.g., connected by a unique id)? **Yes**
	* I3 Is annotation done to the _precision_ commensurate with the theory (e.g., likert scales for dimensions, is there an explicit theory behind annotations)? **Yes**
- **Objectivity**
	* I4 Does the annotation scheme adhere to the theory it follows and can it be interpreted by others in the same way? **Yes**
- **Believability**
	* I5 Is the documentation and the instructions (specific wording that the annotators saw) for the annotation available? **No**
	* I6 Is the overall credibility of the data at the level required (e.g., sufficient power per instance to provide a reliable annotation, no conflicting reporting, or piece-meal or ad-hoc assembly of multiple incompatible or unclear dataset versions)? **No**
- **Reputation**
	* I7 Has the dataset been used by multiple studies by different authors (over 3-5)? **Yes**

##### Accessibility DQ (4)

- **Accessibility** (public availability, discoverability of resources required to reproduce; music availability)
	* A1 Is the dataset publicly available and functionally useable (all the scripts needed to compile separate parts of the data are available)? **Yes**
	* A2 Is the music available? (functionally available, but see next question) **Yes**
- **Access security**
	* A3 If parts of the data (music, lyrics, album covers, etc) are copyrighted, is computerized data accessible with reasonable effort? **NA**
- **Cost effectiveness** (from 1996 paper)
	* What has been the estimated cost of establishing the dataset (annotator pay x time) [non-scored question]
	* A4 Is the value of the data significant in terms of the cost invested (e.g. over $5000) or culture-specific expertise? [scored variant of the question] **Yes**

##### Contextual DQ (6)

- **Relevancy** (width of the coverage: annotation, features, music, lyrics, demographics, etc., what was sampling strategy from GEBRU)
	* C1 Is a sampling strategy for choosing the excerpts provided and explained? **No**
	* C2 Is the scope of the dataset clearly defined and justified (e.g. "Western classical piano music" rather than emotional music)? **No**
	* C3 Is the background of the annotators relevant for the music and tasks annotated and explained? **No**
- **Timeliness**
	* C4 Can the data obtained and used without human intervention in a timely manner? **Yes**
- **Value-added**
	* C5 Does the dataset fill in a clear gap in the field, is it genuinely novel? **Yes**
- **Completeness**
	* C5 Are all the items complete (no missing sub-sections, items, annotations, domains of data)? **Yes**
- **Amount of data**
	* What are the numbers for (1) stimulus N, (2) stimulus length (median), (3) annotator N (if raw data given), (4) N domains of data (eg., annotations, audio, lyrics, features, videos, movement, tags, etc.) [non-scored question]

##### Representational DQ (7)

- **Interpretability & ease of understanding**
	* R1 Are all annotated constructs defined in the documentation? **No**
	* R2 Is the documentation complete and the data human readable? **Yes**
- **Concise representation** (merge/build procedure; preprocessing; outlier removal)
	* R3 Are the building blocks of the dataset explained clearly and the preprocessing steps explained or covered explicitly with scripts/functions? **No**
- **Consistent representation** (what emotion framework is represented and how)
	* R4 Are domain of data (annotations, music, lyrics, tags, features) explained where they come from and how were they processed, extracted and/or compiled? **Yes**
	* R5 Do the emotion annotations represent a known theoretical notion and what framework (affective circumplex, affect quadrants, basic emotions, aesthetic emotions, other)  **Yes**
- **Type of acquisition of data** (crowd-sourced, experts, university students, lay people, gamified, etc.)
	* R6 Are the annotator credentials explained at a sufficient level (crowd-sourced, experts, university students, laypeople, gamified, etc.)? **Yes**
	* R7 Is the _data description_ (see next) fully covered in the documentation? **Yes**

> Score: Intrinsic (5/7), Accessibility (4/4), Contextual (3/6), Representational (5/7)

> **Total Sum**: 5+4+3+5 = 17 out of 24 (7+4+6+7) or 71%

> **Total Relative**: 5/7 + 4/4 + 3/6 + 5/7 = 2.92/4.00 or 73%



