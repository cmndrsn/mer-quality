category,task,prompt
introduce,introduce,"**An evaluation of music emotion recognition datasets** In this interactive module, you will be asked to annotate and evaluate the quality of open-access data used in Music Emotion Recognition (MER) studies. The module draws upon two highly influential papers in the Data Quality literature by Gebru et al. (2019) and Wang & Strong (1996), either adapting or directly adopting questions in an effort to improve documentation surrounding MER datasets."
introduce,introduce,"**An evaluation of music emotion recognition datasets** In the first stage, you will be asked to fill in information surrounding the creators and design of the dataset. These questions come from Datasheets for Datasets by Gebru et al. (2019). In a second stage, you will be asked to rate several data quality dimension items on a scale from 1 = poor to 5 = impeccable."
motivation,annotate,**For what purpose was the dataset created?** Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.
motivation,annotate,"**Who created this dataset?** (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?"
motivation,annotate,"**What support was needed to make this dataset?** (e.g. who funded the creation of the dataset? If there is an associated grant, provide the name of the grantor and the grant name and number, or if it was supported by a company or government agency, give those details.)"
motivation,annotate,**What has been the estimated cost of establishing the dataset (annotator pay x time)? Is the value of the data significant in terms of the cost invested (e.g. over $5000) or culture-specific expertise?**
motivation,annotate,"**Amount of data** What are the numbers for (1) stimulus N, (2) stimulus length (median), (3) annotator N (if raw data given), (4) N domains of data (eg., annotations, audio, lyrics, features, videos, movement, tags, etc.)?"
intrinsic,evaluate,"**The task draws from relevant theories.** Does the task draw from prevailing theories of emotion? Are any compromises made, or inconsistencies introduced, when adapting theory for the research task?"
intrinsic,evaluate,"**The emotion annotations represent a known theoretical notion and what framework (affective circumplex, affect quadrants, basic emotions, aesthetic emotions, other) **"
intrinsic,evaluate,"**Analyses were conducted to assess the reliability of results.** Were additional analyses performed for validation (e.g., interrater reliability, cross-validation?) Does the reliability of the annotations meet the typical norms? (N, expertise, reporting inter-rater agreement, elimination of responses or annotators)"
intrinsic,evaluate,"**Study design decisions provide an impression that the study results are believable and replicable.** Does the combination of decisions about the measurement construct; participant recruitment; stimulus, feature, and model selection lead to an impression that the study conclusions are appropriate and believable?"
intrinsic,evaluate," **All instances are explicitly linked with all forms of data (e.g., connected by a unique id)**"
intrinsic,evaluate,"**The overall credibility of the data meets the expected level** (e.g., sufficient power per instance to provide a reliable annotation, no conflicting reporting, or piece-meal or ad-hoc assembly of multiple incompatible or unclear dataset versions)"
intrinsic,evaluate,**The study appears to be reputable when considering the journal and readership/citation metrics.** Has the dataset been used in other studies by different authors?
intrinsic,evaluate,"**The data description is fully covered in the documentation.** Consider the amount of data, genre(s), domains of data, annotator expertise and culture, annotation task, excerpt duration, source where available"
accessibility,evaluate,"**The data from this project are publicly available.** Do any barriers exist that could impact the reproducibility of the project? (e.g., copyright issues, lack of documentation, poor repository management)."
accessibility,evaluate,**The dataset is functionally useable** (all the scripts needed to compile separate parts of the data are available)
accessibility,evaluate,"**The dataset is self-contained, or links to otherwise external resources (e.g., websites, tweets, other datasets).** If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user?"
accessibility,evaluate,**Do users require special permissions to access the data?** Are costs associated with the creation of the dataset reported?
accessibility,evaluate,"**If parts of the data (music, lyrics, album covers, etc) are copyrighted, computerized data is accessible with reasonable effort.**"
contextual,evaluate,"**Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?** If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable)."
contextual,evaluate,"**If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?**"
contextual,evaluate,"**The dataset fills a gap in the literature.** Does the dataset fill in a clear gap in the field, is it genuinely novel? "
contextual,evaluate,"**Is the public-facing dataset available in partial or complete form?** In other words, is enough data provided that the analyses reported in the accompanying paper can be reproduced?"
contextual,evaluate,"**The scope of the dataset clearly defined and justified (e.g. ""Western classical piano music"" rather than emotional music)?**"
contextual,evaluate,"**All the items complete** (no missing sub-sections, items, annotations, domains of data)"
contextual,evaluate,**The background of the annotators is relevant for the music and tasks annotated.**
representational,evaluate,"**The dataset documentation is complete and clearly understandable.** Is sufficient information provided to understand how data are stored (e.g., repository structure and filenames), represented (column names, data, and metadata), and analyzed (preprocessing/analysis scripts)?"
representational,evaluate,"**The data are represented concisely.** Are data provided in a human-readable format? Are redundant observations, verbose labels, or missing values/outliers accounted for in preprocessing and/or documentation?"
representational,evaluate,"**Measured constructs are represented and applied in a manner that is both internally consistent within the study, and externally consistent with other studies.** In other words, are frameworks employed consistently employed throughout the study, as well as with other studies in the literature?"
representational,evaluate,**The building blocks of the dataset explained clearly and the preprocessing steps explained or covered explicitly with scripts/functions**
representational,evaluate,"**All domains of data (annotations, music, lyrics, tags, features) are explained in terms of where they come from and how were they processed, extracted and/or compiled**"
representational,evaluate,"**What demographic/level of expertise do the annotators represent?** Does the design of study account for differences in expertise, primary language, demographics, recruitment platforms? If so, is this information available to users of the dataset?"
debrief,debrief,"**Thank you** Thank you for your time. Further reading:
Wang, R. Y., & Strong, D. M. (1996). Beyond accuracy: What data quality means to data consumers. Journal of management information systems, 12(4), 5-33.
Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., & Crawford, K. (2021). Datasheets for datasets. Communications of the ACM, 64(12), 86-92."